{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MacroSim Documentation MacroSim is a python library aimed at creating symbloic models of economic variables thgough PySR's PySRRegressor . It utilizes a FRED series accessor and a cunstomizable equation search engine to find an accurate smybolic representation of the selected variavle using the features retrieved from FRED series. MacroSim contains a simulation engine that has the capability to extrapolate the given data points using fully symbolic, per-variable growth rate equations. MacroSim's main focus is not on producing the most accurate output, but to ensure explorability of outputs for research purposes. Both the symbolic regression results and the fitted growth equations prioritize interpretability. Although kinks are often produced in the output extrapolation process, the equations themselves are configured to be differenciable in most cases. Installation MacroSim can be installed through pip ; and the builds are available in the github repo if you prefer to install it manually. The pip command required to retireve MacroSim is: python -m pip install macrosim Example Usage Production function modelling (I believe) can be one of the most common use cases for MacroSim, and it will be the example of choice here. Let's assume we're looking into modelling real GDP as a function of: Labor Participation Rate Capital Investment Net Exports CPI Population Growth Real Wage Data Retrieval The first step would be to retrieve data for the above-mentioned metrics from FRED using macrosim.SeriesAccessor . from macrosim.SeriesAccessor import SeriesAccessor import datetime as dt fred = SeriesAccessor( key_path='../fred_key.env', key_name='FRED_KEY' ) start = dt.datetime.fromisoformat('2002-01-01') end = dt.datetime.fromisoformat('2024-01-01') df = fred.get_series( series_ids = ['NETEXP', 'CIVPART', 'CORESTICKM159SFRBATL', 'LES1252881600Q', 'SPPOPGROWUSA', 'A264RX1A020NBEA', 'GDPC1'], series_alias=[None, None, 'CPI', 'RWAGE', 'POPGROWTH', 'I_C', 'RGDP'], reindex_freq='QS', date_range=(start, end), ) reindex_freq='QS' reformats all data to quarterly frequency, introducing NaN values if the original series is updated less frequently. The reindexing operation matches the frequency of all variables to the target. (being real GDP) We can deal with the NaN values, again, using macrosim.SeriesAccessor . df = fred.fill( data=df, methods=[None, None, None, None, 'ffill', 'divide', None] ) In the example, only 2 of the series had data that's less frequent than quarterly. Here, the elected built-in fill methods are specified in string literals. (refer to MacroSim Docs for detailed information) We now have a dataset free of NaN s and ready for symbolic regression Symbolic Regression The symbolic regression backend of MacroSim relies on the PySR library which provides a regressor written in julia, (a compiled language) and a python interface. macrosim.EqSearch is a class that takes the pysr.PySRRegressor as its base and extends it by including model distillation and LOF outlier detection features. (Reasons behind opting for distillation and LOF based outlier removal are discussed further below) Having our dataset, we can conduct a symbolic search to derive the most accurate representation of real GDP within the constraints that we define. from macrosim.EqSearch import EqSearch eqsr = EqSearch( X= df.drop('RGDP', axis=1), y= df['RGDP'] ) eqsr.distil_split() eqsr.search() eq = eqsr.eq EqSearch.distil_split was called to filter out the outliers with LOF and distil the target variable through a Random Forest regressor. The aggressive outlier handling implemented ensures that the symbolic regression will target a more general scope instead of attempting to predict shocks at fit time. The following call to EqSearch.search initiates the symbolic regression; no constraints were defined here, but you can refer to the documentation for a detailed explanation of how to set them. We now have the core component for our simulation engine, the main equation that will be responsible for mapping our inputs to outputs. However, we are yet to explore the growth patterns of our input variables. We need to define functions that will govern how our inputs evolve over \\(n\\) steps of simulation. Input Growth Modelling [Proof of Concept!!] macrosim.GrowthDetector is responsible for deriving growth functions for each input variable. Using the historical data we have, pre-defined parametrized growth functions are fitted to the data and the best fitting functions are selected for each variable on the basis of MSE. from macrosim.GrowthDetector import GrowthDetector gd = GrowthDetector() opt = gd.find_opt_growth(df.drop('RGDP', axis=1)) print(opt) >>> {'NETEXP': (Logarithmic(x, 579.61, 0.0, 1.0, 170.21), MSE = 20050.11), 'CIVPART': (Linear(x, -0.06), MSE = 0.38), 'CPI': (Logarithmic(x, 1.85, 1.13, 1.0, 0.03), MSE = 1.22), 'RWAGE': (Exponential(x, 1.0, 0.0), MSE = 119.37), 'POPGROWTH': (Exponential(x, 1.02, -0.02), MSE = 0.02), 'I_C': (Exponential(x, 1.02, -1.46), MSE = 409.61)} We can see that GrowthDetector.find_opt_growth returns a dictionary of tuples. Index 0 of the tuples contain a function with a custom __repr__ method to signify the nature and parameters of the function that best describes the growth of said variable. Index 1 contains the MSE value ( np.float64 ) obtained at fit time. (again, with a formatted __repr__ method) This gives us all we need to initiate the simulation. Simulating the derived scenario The simulation engine is built as a non-exhaustible generator that will recursively apply the growth functions and derive an output using the symbolic regression result. from macrosim.SimEngine import SimEngine initial_params = { col: (df[col].iloc[0], opt[col][0]) for col in df.columns[:-1] } engine = SimEngine( eq=eq, init_params=initial_params, deterministic=True, entropy_coef=0.01 ) for _ in range(50): next(engine._simulate()) output_df = engine.get_history() As the first step, we defined the initial values and growth functions that will govern them as a tuple for each column. will be inputted to the generator logic. We used the first index of our historical data (the most recent observations) to be able to compare our results with the real observations. (We could've used the most recent observations to simulate future periods.) Then, an instance of macrosim.SimEngine was declared with the necessary parameters. The chosen entropy coefficient was 0.01 (this value defines the average extent of noise as a percentage of the variables) so we expect to see shocks (noise) that's 1% of the values computed by the growth functions in both positive and negative directions. Using entropy_coef=0 will completely disable the noise and will allow a clean inspection of growth function behavior. A Peak at the Results By plotting the simulated data against the real observations, we can check the performance. Due to insufficient accuracy of growth functions (for now) accurate results are not expected. Plotting the simulation confirms this: We can see that the results were relatively accurate until the 2008 crisis. (it honestly performed better than I expected) The growth functions are called recursively, and we used a minimal amount of randomized entropy. This meant that we didn't stand a chance against a massive shock. The engine simply preserved the expected state as defined by the symbolic expressions. Comments As seen in the example, the current state of MacroSim is not well suited for stochastic shocks. After the implementation of more advanced growth modelling techniques, this issue will also be addressed. The current example showed that a symbolic regression and simulation pipeline has the power to (at the very least) preserve the state of an economy over time. As improvements and additions are implemented, MacroSim can become a gateway to utilise a regression model that was built for natural sciences and take it to the realm of economic simulation.","title":"Home"},{"location":"#macrosim-documentation","text":"MacroSim is a python library aimed at creating symbloic models of economic variables thgough PySR's PySRRegressor . It utilizes a FRED series accessor and a cunstomizable equation search engine to find an accurate smybolic representation of the selected variavle using the features retrieved from FRED series. MacroSim contains a simulation engine that has the capability to extrapolate the given data points using fully symbolic, per-variable growth rate equations. MacroSim's main focus is not on producing the most accurate output, but to ensure explorability of outputs for research purposes. Both the symbolic regression results and the fitted growth equations prioritize interpretability. Although kinks are often produced in the output extrapolation process, the equations themselves are configured to be differenciable in most cases.","title":"MacroSim Documentation"},{"location":"#installation","text":"MacroSim can be installed through pip ; and the builds are available in the github repo if you prefer to install it manually. The pip command required to retireve MacroSim is: python -m pip install macrosim","title":"Installation"},{"location":"#example-usage","text":"Production function modelling (I believe) can be one of the most common use cases for MacroSim, and it will be the example of choice here. Let's assume we're looking into modelling real GDP as a function of: Labor Participation Rate Capital Investment Net Exports CPI Population Growth Real Wage","title":"Example Usage"},{"location":"#data-retrieval","text":"The first step would be to retrieve data for the above-mentioned metrics from FRED using macrosim.SeriesAccessor . from macrosim.SeriesAccessor import SeriesAccessor import datetime as dt fred = SeriesAccessor( key_path='../fred_key.env', key_name='FRED_KEY' ) start = dt.datetime.fromisoformat('2002-01-01') end = dt.datetime.fromisoformat('2024-01-01') df = fred.get_series( series_ids = ['NETEXP', 'CIVPART', 'CORESTICKM159SFRBATL', 'LES1252881600Q', 'SPPOPGROWUSA', 'A264RX1A020NBEA', 'GDPC1'], series_alias=[None, None, 'CPI', 'RWAGE', 'POPGROWTH', 'I_C', 'RGDP'], reindex_freq='QS', date_range=(start, end), ) reindex_freq='QS' reformats all data to quarterly frequency, introducing NaN values if the original series is updated less frequently. The reindexing operation matches the frequency of all variables to the target. (being real GDP) We can deal with the NaN values, again, using macrosim.SeriesAccessor . df = fred.fill( data=df, methods=[None, None, None, None, 'ffill', 'divide', None] ) In the example, only 2 of the series had data that's less frequent than quarterly. Here, the elected built-in fill methods are specified in string literals. (refer to MacroSim Docs for detailed information) We now have a dataset free of NaN s and ready for symbolic regression","title":"Data Retrieval"},{"location":"#symbolic-regression","text":"The symbolic regression backend of MacroSim relies on the PySR library which provides a regressor written in julia, (a compiled language) and a python interface. macrosim.EqSearch is a class that takes the pysr.PySRRegressor as its base and extends it by including model distillation and LOF outlier detection features. (Reasons behind opting for distillation and LOF based outlier removal are discussed further below) Having our dataset, we can conduct a symbolic search to derive the most accurate representation of real GDP within the constraints that we define. from macrosim.EqSearch import EqSearch eqsr = EqSearch( X= df.drop('RGDP', axis=1), y= df['RGDP'] ) eqsr.distil_split() eqsr.search() eq = eqsr.eq EqSearch.distil_split was called to filter out the outliers with LOF and distil the target variable through a Random Forest regressor. The aggressive outlier handling implemented ensures that the symbolic regression will target a more general scope instead of attempting to predict shocks at fit time. The following call to EqSearch.search initiates the symbolic regression; no constraints were defined here, but you can refer to the documentation for a detailed explanation of how to set them. We now have the core component for our simulation engine, the main equation that will be responsible for mapping our inputs to outputs. However, we are yet to explore the growth patterns of our input variables. We need to define functions that will govern how our inputs evolve over \\(n\\) steps of simulation.","title":"Symbolic Regression"},{"location":"#input-growth-modelling-proof-of-concept","text":"macrosim.GrowthDetector is responsible for deriving growth functions for each input variable. Using the historical data we have, pre-defined parametrized growth functions are fitted to the data and the best fitting functions are selected for each variable on the basis of MSE. from macrosim.GrowthDetector import GrowthDetector gd = GrowthDetector() opt = gd.find_opt_growth(df.drop('RGDP', axis=1)) print(opt) >>> {'NETEXP': (Logarithmic(x, 579.61, 0.0, 1.0, 170.21), MSE = 20050.11), 'CIVPART': (Linear(x, -0.06), MSE = 0.38), 'CPI': (Logarithmic(x, 1.85, 1.13, 1.0, 0.03), MSE = 1.22), 'RWAGE': (Exponential(x, 1.0, 0.0), MSE = 119.37), 'POPGROWTH': (Exponential(x, 1.02, -0.02), MSE = 0.02), 'I_C': (Exponential(x, 1.02, -1.46), MSE = 409.61)} We can see that GrowthDetector.find_opt_growth returns a dictionary of tuples. Index 0 of the tuples contain a function with a custom __repr__ method to signify the nature and parameters of the function that best describes the growth of said variable. Index 1 contains the MSE value ( np.float64 ) obtained at fit time. (again, with a formatted __repr__ method) This gives us all we need to initiate the simulation.","title":"Input Growth Modelling [Proof of Concept!!]"},{"location":"#simulating-the-derived-scenario","text":"The simulation engine is built as a non-exhaustible generator that will recursively apply the growth functions and derive an output using the symbolic regression result. from macrosim.SimEngine import SimEngine initial_params = { col: (df[col].iloc[0], opt[col][0]) for col in df.columns[:-1] } engine = SimEngine( eq=eq, init_params=initial_params, deterministic=True, entropy_coef=0.01 ) for _ in range(50): next(engine._simulate()) output_df = engine.get_history() As the first step, we defined the initial values and growth functions that will govern them as a tuple for each column. will be inputted to the generator logic. We used the first index of our historical data (the most recent observations) to be able to compare our results with the real observations. (We could've used the most recent observations to simulate future periods.) Then, an instance of macrosim.SimEngine was declared with the necessary parameters. The chosen entropy coefficient was 0.01 (this value defines the average extent of noise as a percentage of the variables) so we expect to see shocks (noise) that's 1% of the values computed by the growth functions in both positive and negative directions. Using entropy_coef=0 will completely disable the noise and will allow a clean inspection of growth function behavior.","title":"Simulating the derived scenario"},{"location":"#a-peak-at-the-results","text":"By plotting the simulated data against the real observations, we can check the performance. Due to insufficient accuracy of growth functions (for now) accurate results are not expected. Plotting the simulation confirms this: We can see that the results were relatively accurate until the 2008 crisis. (it honestly performed better than I expected) The growth functions are called recursively, and we used a minimal amount of randomized entropy. This meant that we didn't stand a chance against a massive shock. The engine simply preserved the expected state as defined by the symbolic expressions.","title":"A Peak at the Results"},{"location":"#comments","text":"As seen in the example, the current state of MacroSim is not well suited for stochastic shocks. After the implementation of more advanced growth modelling techniques, this issue will also be addressed. The current example showed that a symbolic regression and simulation pipeline has the power to (at the very least) preserve the state of an economy over time. As improvements and additions are implemented, MacroSim can become a gateway to utilise a regression model that was built for natural sciences and take it to the realm of economic simulation.","title":"Comments"},{"location":"class_docs/BaseVarSelector/","text":"BaseVarSelector BaseVarSelector handles variable selection for constructing the base of the feedback loop that models the variables' growth in the simulation process. It uses two implementations of Granger Causality Tests to determine the variables with the strongest causality over the whole feature set. The tests are of similar fashion with one being a Bivariate GCT and the other being a Multivariate GCT . Both tests are structured tho check statistical significance of the statement \\(H_0: x \\underset{\\text{Granger-cause}}{\\not\\to} z\\) and \\(H_\\alpha: x\\underset{\\text{Granger-cause}}{\\to} z\\) . As any classical hypothesis test, we check of the p-value of \\(H_0\\) . Lower p-values indicate greater statistical evidence against \\(H_0\\) , suggesting that x Granger-causes z . Importantly, GCT checks for linear causality so the significance we're computing is for the probability of a function existing such that: \\[ f(x_t, x_{t-1}, \\dots, x_{t-n}) = z_t \\] \\[ \\text{and} \\] \\[ f(x_t. x_{t-1}, \\dots, x_{t-n}) = a_0x_t + a_1x_{t-1}+\\dots+a_nx_{t-n} \\] The multivariate case follow the same pattern and test if \\((x, y) \\underset{Granger-cause}{\\to}z\\) . Unfortunately, we're only testing for the existence of a function that satisfies the conditions defined above, therefore a multivariate test will not provide context regarding which variable is more significant in creating causality. BaseVarSelector ranks variables (or pairs of variables) based on their p-values instead of using a threshold of significance to assert causality. This ensures that the best performing variables will always be selected as opposed tests returning no eligible variables for a given feature set. Example Usage from macrosim import BaseVarSelector import pandas as pd df = pd.DataFrame(...) bvs = BaseVarSelector(df=df) bvs.granger_matrix(score=True) # Compute a matrix of p-values for all combinations of Bivariate GCTs; # record variable ranks on average p-value if score=True bvs.multivar_granger_matrix() # Compute p-values for all possible combinations of 2-predictor GCTs; # record best variable ranks by average p-values of all pairs they've been a part of. # (No matrix returned as there the raw data is often too large to visualise) print(bvs.score_dict) # Print a dict listing the ranks of variables in terms of their performance (separate for both tests) overall_score = { k: (2/3)*v['Granger'] + (1/3)*v['Multivar_Granger'] for k, v in bvs.score_dict } # Weighted sum of scores from both tests for each variable # Lower is better Methods BaseVarSelector.granger_matrix Computes a matrix of p-values for every possible Bivariate GCT of the inputted data. Params: score: bool : Record the ranks of variables into BaseVarSelector.score_dict if True. Returns: pd.DataFrame : Matrix of p-values for all GCTs computed. BaseVarSelector.multivar_granger_matrix Computes all Multivariate GCTs with two predictors ( \\((x,y) \\underset{\\text{Granger-cause}}{\\to}z\\) ) and records the variable ranks to BaseVarSelector.score_dict . Params: None Returns: None","title":"BaseVarSelector"},{"location":"class_docs/BaseVarSelector/#basevarselector","text":"BaseVarSelector handles variable selection for constructing the base of the feedback loop that models the variables' growth in the simulation process. It uses two implementations of Granger Causality Tests to determine the variables with the strongest causality over the whole feature set. The tests are of similar fashion with one being a Bivariate GCT and the other being a Multivariate GCT . Both tests are structured tho check statistical significance of the statement \\(H_0: x \\underset{\\text{Granger-cause}}{\\not\\to} z\\) and \\(H_\\alpha: x\\underset{\\text{Granger-cause}}{\\to} z\\) . As any classical hypothesis test, we check of the p-value of \\(H_0\\) . Lower p-values indicate greater statistical evidence against \\(H_0\\) , suggesting that x Granger-causes z . Importantly, GCT checks for linear causality so the significance we're computing is for the probability of a function existing such that: \\[ f(x_t, x_{t-1}, \\dots, x_{t-n}) = z_t \\] \\[ \\text{and} \\] \\[ f(x_t. x_{t-1}, \\dots, x_{t-n}) = a_0x_t + a_1x_{t-1}+\\dots+a_nx_{t-n} \\] The multivariate case follow the same pattern and test if \\((x, y) \\underset{Granger-cause}{\\to}z\\) . Unfortunately, we're only testing for the existence of a function that satisfies the conditions defined above, therefore a multivariate test will not provide context regarding which variable is more significant in creating causality. BaseVarSelector ranks variables (or pairs of variables) based on their p-values instead of using a threshold of significance to assert causality. This ensures that the best performing variables will always be selected as opposed tests returning no eligible variables for a given feature set.","title":"BaseVarSelector"},{"location":"class_docs/BaseVarSelector/#example-usage","text":"from macrosim import BaseVarSelector import pandas as pd df = pd.DataFrame(...) bvs = BaseVarSelector(df=df) bvs.granger_matrix(score=True) # Compute a matrix of p-values for all combinations of Bivariate GCTs; # record variable ranks on average p-value if score=True bvs.multivar_granger_matrix() # Compute p-values for all possible combinations of 2-predictor GCTs; # record best variable ranks by average p-values of all pairs they've been a part of. # (No matrix returned as there the raw data is often too large to visualise) print(bvs.score_dict) # Print a dict listing the ranks of variables in terms of their performance (separate for both tests) overall_score = { k: (2/3)*v['Granger'] + (1/3)*v['Multivar_Granger'] for k, v in bvs.score_dict } # Weighted sum of scores from both tests for each variable # Lower is better","title":"Example Usage"},{"location":"class_docs/BaseVarSelector/#methods","text":"","title":"Methods"},{"location":"class_docs/BaseVarSelector/#basevarselectorgranger_matrix","text":"Computes a matrix of p-values for every possible Bivariate GCT of the inputted data. Params: score: bool : Record the ranks of variables into BaseVarSelector.score_dict if True. Returns: pd.DataFrame : Matrix of p-values for all GCTs computed.","title":"BaseVarSelector.granger_matrix"},{"location":"class_docs/BaseVarSelector/#basevarselectormultivar_granger_matrix","text":"Computes all Multivariate GCTs with two predictors ( \\((x,y) \\underset{\\text{Granger-cause}}{\\to}z\\) ) and records the variable ranks to BaseVarSelector.score_dict . Params: None Returns: None","title":"BaseVarSelector.multivar_granger_matrix"},{"location":"class_docs/EqSearch/","text":"EqSearch EqSearch is responsible with applying symbolic regression to derive target equations for simulation. It utilizes pysr's PySRRegressor model, which is written and pre-compiled in julia. As an important note, pysr is currently in development and UnicodeDecodeError s are common during equation searches. (only tested in ipython environments) Although raised, these byte decode errors do not interrupt the runtime and better ipython support will likely be implemented at some point. To generalize the output expressions, an aggressive outlier elimination process is applied in EqSearch . This increases the chances of getting differentiable and interpretable outputs. n-neighbor based outlier detection is implemented with sklearn 's LocalOutlierFactor model and after the LOF based outlier elimination, the remaining data is further distilled through the use of a RandomForestRegressor . Following this procedure, PySRRegressor is utilized to conduct an iterative search for the best fitting symbolic expression within the user-defined constraints. Example Usage from macrosim import EqSearch from pandas import DataFrame, Series from sympy import sin x: DataFrame = ... y: Series | DataFrame = ... eqsr = EqSearch( X=x, # features y=y, # label random_state=0, model_selection='best' ) eqsr.distil_split(grid_search=False) # Model distillation (required step) eqsr.search( extra_unary_ops={ 'sin': { 'julia': 'sin', 'sympy': lambda x: sin(x) } }, constraints={'sin': 2, '^': (-1, 1)} # Complexity limit of terms in binary and unary operations ) eq = eqsr.eq Methods EqSearch.distil_split Applies outlier handling and model distillation. Params: - test_size: float : test size used for the data split of RandomForestRegressor . - grid_search: bool : Conduct cross-validated grid search for RandomForestRegressor tuning if True. - gs_params: dict[str, list[Any]] : Param grid to use if grid_search=True . Returns: - None EqSearch.search Uses PySRRegressor to conduct a symbolic expression search. Calling search before distil_split will raise an AssertionError . Records the resulting equation as a sympy expression to self.eq . Params: - binary_ops: tuple[str] : tuple of binary operators (as strings) allowed in the final expression. Defaults to use all binary operators available. unary_ops: tuple[str] : tuple of unary operators allowed in the final expression. Defaults to ('exp', 'log', 'sqrt') . extra_unary_ops: dict[str, dict[str, Any]] : dict of unary ops that are not built-in to python, sympy, or julia. Every search will include the extra unary: {'inv': {julia: 'inv(x)=1/x', 'sympy': lambda x: 1/x} . For each operator, a julia and sympy applicable definition is necessary. julia implementations are passed as strings in julia syntax, to be parsed on the PySRRegressor side. custom_loss: str : elementwise loss function to use, either written in julia syntax or a string literal for predefined loss functions, available in PySR documentation . Defaults to 'L2DistLoss()'. (sum of square difference, similar to MSE) constraints: dict[str, Union[int, tuple[int, int]] : constraints to expression complexity of binary and unary operations. Complexity, in this case, refers to the amount of operations required to reduce an input to its simplest form. For example, 1+1 can be reduced to it's simplest form in one addition, making it's overall complexity equal to 1 while a*1+1 would require at least 2 operations, so it has a complexity of 2. (assuming a is a scalar) For unary operations, an integer is passed to define how complex of an input can be used. For example, a complexity of 1 would only allow a single variable or constant. With a complexity of 1, the sin operator would only be used as sin(x) while a complexity of 2 would for example allow sin(x+C) . Binary operators function similarly, but their constraints are defined as a tuple of integers, for the inputs x , and y of the operation. For example, a constraint of (1, 1) on the ^ operator would only allow expressions of type x^y while a constraint of (2, 1) would allow (x+1)^y . Returns: - None (Results saved to self.eq )","title":"EqSearch"},{"location":"class_docs/EqSearch/#eqsearch","text":"EqSearch is responsible with applying symbolic regression to derive target equations for simulation. It utilizes pysr's PySRRegressor model, which is written and pre-compiled in julia. As an important note, pysr is currently in development and UnicodeDecodeError s are common during equation searches. (only tested in ipython environments) Although raised, these byte decode errors do not interrupt the runtime and better ipython support will likely be implemented at some point. To generalize the output expressions, an aggressive outlier elimination process is applied in EqSearch . This increases the chances of getting differentiable and interpretable outputs. n-neighbor based outlier detection is implemented with sklearn 's LocalOutlierFactor model and after the LOF based outlier elimination, the remaining data is further distilled through the use of a RandomForestRegressor . Following this procedure, PySRRegressor is utilized to conduct an iterative search for the best fitting symbolic expression within the user-defined constraints.","title":"EqSearch"},{"location":"class_docs/EqSearch/#example-usage","text":"from macrosim import EqSearch from pandas import DataFrame, Series from sympy import sin x: DataFrame = ... y: Series | DataFrame = ... eqsr = EqSearch( X=x, # features y=y, # label random_state=0, model_selection='best' ) eqsr.distil_split(grid_search=False) # Model distillation (required step) eqsr.search( extra_unary_ops={ 'sin': { 'julia': 'sin', 'sympy': lambda x: sin(x) } }, constraints={'sin': 2, '^': (-1, 1)} # Complexity limit of terms in binary and unary operations ) eq = eqsr.eq","title":"Example Usage"},{"location":"class_docs/EqSearch/#methods","text":"","title":"Methods"},{"location":"class_docs/EqSearch/#eqsearchdistil_split","text":"Applies outlier handling and model distillation. Params: - test_size: float : test size used for the data split of RandomForestRegressor . - grid_search: bool : Conduct cross-validated grid search for RandomForestRegressor tuning if True. - gs_params: dict[str, list[Any]] : Param grid to use if grid_search=True . Returns: - None","title":"EqSearch.distil_split"},{"location":"class_docs/EqSearch/#eqsearchsearch","text":"Uses PySRRegressor to conduct a symbolic expression search. Calling search before distil_split will raise an AssertionError . Records the resulting equation as a sympy expression to self.eq . Params: - binary_ops: tuple[str] : tuple of binary operators (as strings) allowed in the final expression. Defaults to use all binary operators available. unary_ops: tuple[str] : tuple of unary operators allowed in the final expression. Defaults to ('exp', 'log', 'sqrt') . extra_unary_ops: dict[str, dict[str, Any]] : dict of unary ops that are not built-in to python, sympy, or julia. Every search will include the extra unary: {'inv': {julia: 'inv(x)=1/x', 'sympy': lambda x: 1/x} . For each operator, a julia and sympy applicable definition is necessary. julia implementations are passed as strings in julia syntax, to be parsed on the PySRRegressor side. custom_loss: str : elementwise loss function to use, either written in julia syntax or a string literal for predefined loss functions, available in PySR documentation . Defaults to 'L2DistLoss()'. (sum of square difference, similar to MSE) constraints: dict[str, Union[int, tuple[int, int]] : constraints to expression complexity of binary and unary operations. Complexity, in this case, refers to the amount of operations required to reduce an input to its simplest form. For example, 1+1 can be reduced to it's simplest form in one addition, making it's overall complexity equal to 1 while a*1+1 would require at least 2 operations, so it has a complexity of 2. (assuming a is a scalar) For unary operations, an integer is passed to define how complex of an input can be used. For example, a complexity of 1 would only allow a single variable or constant. With a complexity of 1, the sin operator would only be used as sin(x) while a complexity of 2 would for example allow sin(x+C) . Binary operators function similarly, but their constraints are defined as a tuple of integers, for the inputs x , and y of the operation. For example, a constraint of (1, 1) on the ^ operator would only allow expressions of type x^y while a constraint of (2, 1) would allow (x+1)^y . Returns: - None (Results saved to self.eq )","title":"EqSearch.search"},{"location":"class_docs/GrowthPatternDetector/","text":"GrowthPatternDetector Docs will be written when the first beta release of SimEngine is out.","title":"GrowthPatternDetector"},{"location":"class_docs/GrowthPatternDetector/#growthpatterndetector","text":"Docs will be written when the first beta release of SimEngine is out.","title":"GrowthPatternDetector"},{"location":"class_docs/SeriesAccessor/","text":"SeriesAccessor SeriesAccessor is a wrapper class built around the fredapi library to provide additional utilities that come in handy when working with time series of varying frequencies at the same time. Retrieving data through this class is not a necessity and EqSearch will essentially work with any arbitrary dataset that's suitable to regression. Also, note that you'll need a free API key to access FRED series, you can follow this article to get one. Example Usage from macrosim import SeriesAccessor import datetime as dt fred = SeriesAccessor( key_path='./key.env', # The class expects an .env file, the is loaded through environment variables key_name='fred_key' # Name of the variable holding the API key in the .env file. ) start = dt.datetime.fromisoformat(\"2000-01-01\") end = dt.datetime.fromisoformat(\"2024-01-01\") df = fred.get_series(series_ids=['CPIAUCSL', 'A264RX1A020NBEA', 'PSAVERT', 'M2REAL', 'GDPC1'], # FRED IDs of the series date_range=(start, end), reindex_freq='QS', # Chosen time series frequency to reindex all data (defaults to max frequency available in the data series_alias=['CPI', 'CAPINV','SRATE', 'M_2','RGDP']) # Col names to use in the output (this will change how variables are represented in equations) df = fred.fill( df, [None, 'ffill', None, None, None] # Provide None, a built-in fill method, or a unary lambda function per column to fill the NaN values produced at reindexing ) Methods SeriesAccessor includes two user-facing methods and it's only functionality is to retrieve and format series from FRED. SeriesAccessor.get_series Retrieves the specified FRED series and applies the specified formatting. Params: series_ids: Sequence[str] : List of FRED series IDs to retireve date_range: tuple(dt.datetime.date, dt.datetime.date) : Date interval to retrieve data for. reindex_freq: str : String literal of the observation frequency to reformat the series series_alias: Sequence[str] : List of aliases to use as column names in the outputted dataframe. EqSearch will use these as the variable names when outputting symbolic equations. Returns: pd.DataFrame : Concatenated and re-indexed dataframe of the series specified SeriesAccessor.fill Fills the given data with specified fill methods. Params: df: pd.DataFrame : DataFrame object to apply the fill methods fill_methods : a list of fill methods for each column in the dataframe. The default behavior is to fill the list with None s until the amount of methods match the mount of columns in the data. The methods can be a unary lambda function, or one of the built-in fill methods passed as a string. The available methods are: ffill : forward-fill, behaves exactly same as pd.Series.ffill . bfill : backward-fill, behaves exactly same as pd.Series.bfill . divide : Divide the last known value equally to \\(n\\) NaN values encountered before the next available row. For example, this can be used to evenly split yearly data to a monthly or quarterly observation frequency. mean : Use the mean to fill NaNs. median : Use the median to fill NaNs. IQR_mean : Use the mean calculated from data within the IQR to fill NaNs. Returns: pd.DataFrame : Dataframe with NaN values filled.","title":"SeriesAccessor"},{"location":"class_docs/SeriesAccessor/#seriesaccessor","text":"SeriesAccessor is a wrapper class built around the fredapi library to provide additional utilities that come in handy when working with time series of varying frequencies at the same time. Retrieving data through this class is not a necessity and EqSearch will essentially work with any arbitrary dataset that's suitable to regression. Also, note that you'll need a free API key to access FRED series, you can follow this article to get one.","title":"SeriesAccessor"},{"location":"class_docs/SeriesAccessor/#example-usage","text":"from macrosim import SeriesAccessor import datetime as dt fred = SeriesAccessor( key_path='./key.env', # The class expects an .env file, the is loaded through environment variables key_name='fred_key' # Name of the variable holding the API key in the .env file. ) start = dt.datetime.fromisoformat(\"2000-01-01\") end = dt.datetime.fromisoformat(\"2024-01-01\") df = fred.get_series(series_ids=['CPIAUCSL', 'A264RX1A020NBEA', 'PSAVERT', 'M2REAL', 'GDPC1'], # FRED IDs of the series date_range=(start, end), reindex_freq='QS', # Chosen time series frequency to reindex all data (defaults to max frequency available in the data series_alias=['CPI', 'CAPINV','SRATE', 'M_2','RGDP']) # Col names to use in the output (this will change how variables are represented in equations) df = fred.fill( df, [None, 'ffill', None, None, None] # Provide None, a built-in fill method, or a unary lambda function per column to fill the NaN values produced at reindexing )","title":"Example Usage"},{"location":"class_docs/SeriesAccessor/#methods","text":"SeriesAccessor includes two user-facing methods and it's only functionality is to retrieve and format series from FRED.","title":"Methods"},{"location":"class_docs/SeriesAccessor/#seriesaccessorget_series","text":"Retrieves the specified FRED series and applies the specified formatting. Params: series_ids: Sequence[str] : List of FRED series IDs to retireve date_range: tuple(dt.datetime.date, dt.datetime.date) : Date interval to retrieve data for. reindex_freq: str : String literal of the observation frequency to reformat the series series_alias: Sequence[str] : List of aliases to use as column names in the outputted dataframe. EqSearch will use these as the variable names when outputting symbolic equations. Returns: pd.DataFrame : Concatenated and re-indexed dataframe of the series specified","title":"SeriesAccessor.get_series"},{"location":"class_docs/SeriesAccessor/#seriesaccessorfill","text":"Fills the given data with specified fill methods. Params: df: pd.DataFrame : DataFrame object to apply the fill methods fill_methods : a list of fill methods for each column in the dataframe. The default behavior is to fill the list with None s until the amount of methods match the mount of columns in the data. The methods can be a unary lambda function, or one of the built-in fill methods passed as a string. The available methods are: ffill : forward-fill, behaves exactly same as pd.Series.ffill . bfill : backward-fill, behaves exactly same as pd.Series.bfill . divide : Divide the last known value equally to \\(n\\) NaN values encountered before the next available row. For example, this can be used to evenly split yearly data to a monthly or quarterly observation frequency. mean : Use the mean to fill NaNs. median : Use the median to fill NaNs. IQR_mean : Use the mean calculated from data within the IQR to fill NaNs. Returns: pd.DataFrame : Dataframe with NaN values filled.","title":"SeriesAccessor.fill"},{"location":"class_docs/SimEngine/","text":"SimEngine Docs will be written when the first beta release of SimEngine is out.","title":"SimEngine"},{"location":"class_docs/SimEngine/#simengine","text":"Docs will be written when the first beta release of SimEngine is out.","title":"SimEngine"}]}